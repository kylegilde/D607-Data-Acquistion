---
title: "DATA 607 Final Project: Bicycles and Wealth in Chicago"
author: "Kyle Gilde"
date: "May 6, 2017"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
    toc_depth: 2

---
```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(
                      warning = FALSE
                      ,error = FALSE
                      ,message = FALSE
                      ,tidy = TRUE
                      ,cache = TRUE
                      )
```

# Research Question
**Are the Divvy bike-share stations disproportionately located in Chicago's wealthier neighborhoods/zip codes?**
Motivation...
http://chi.streetsblog.org/2015/09/10/divvy-membership-skews-white-and-wealthy-but-hopefully-not-for-long/

#Introduction

#Hypotheses
$H_0: B_1 = 0$ **There is no correlation between the number of Divvy station docks and median cost of homes in a neighborhood.**

$H_A: B_1 > 0$ **There is a positive correlation between the number of Divvy station docks and the median cost of homes in a neighborhood.**



#The Variables & Sources of Data

**Loaded Packages**
```{r load_packages, echo = FALSE, comment = ""} 
#create vector with all needed packages
load_packages <- c("prettydoc", "knitr", "jsonlite", "ggmap", "tidyverse", "stringr", "rvest", "psych")

#see if we need to install any of them
install_load <- function(pkg){
  #CODE SOURCE: https://gist.github.com/stevenworthington/3178163
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}

#excute function and display the loaded packages
data.frame(t(t(install_load(load_packages))), fix.empty.names = FALSE)

#(.packages())
```

##The Explanatory Variable: Sell Prices from Trulia.com
The median cost of single-family homes for sale on Trulia.com for each neighborhood/zip code will be used as a proxy for the wealth of the neighborhood/zip code. This was the first publicly available data I found on the web.

 as a proxy to determine the wealth of the neighborhood/zip code. The data will be scraped off of the 300 browse pages and is contained in some JSON on each page. The site appears to have more than 16K homes for sale in Chicago.

The properties that were bank-owned or in default were not included in this study because their sell prices were not publically listed on the site.

###Web Scraping
```{r trulia}

#Inputs to loop
base_url <- "https://www.trulia.com/for_sale/Chicago,IL/SINGLE-FAMILY_HOME_type/"
pages <- 150
trulia_file <- "Trulia_file.csv"
aggregate_df <- data.frame()
reg_ex1 <- "var appState = "
reg_ex2 <- ";\\n  var googleMapURL ="
my_samp <- seq(1, 3, by = .01)

#Loop to scrape pages
if (!trulia_file %in% list.files(getwd())){
  for (i in 1:pages){
    #pagination
    current_url <- ifelse(i == 1,
                          base_url,
                          paste0(base_url, i, "_p/")
                          )
    #get html    
    trulia_html <- current_url %>%
    read_html() %>%
    html_nodes("script") %>%
    html_text()
    
    #get json from html
    json_text <- trulia_html[str_detect(trulia_html, reg_ex1)]
    begin <- as.integer(str_locate(json_text, reg_ex1)[1, 2])
    ending <- as.integer(str_locate(json_text, reg_ex2)[1, 1]) - 1
    
    #parse the JSON
    json <- json_text %>%
      str_sub(begin, ending) %>%
      str_trim() %>%
      fromJSON()
  
    #store data in DF  
    current_df <- data.frame(iteration = i,
                            id = json$page$cards$id,
                            price = json$page$cards$price,
                            zip_code = json$page$cards$zip,
                            location = json$page$cards$footer$location)
  
    aggregate_df <- rbind(aggregate_df, current_df)
  
    #delay
    rand_delay <- sample(my_samp, 1, replace = T)
    Sys.sleep(rand_delay)
  }
  write.csv(aggregate_df, file = trulia_file)
}  
  
```

###Clean and summarize the scraped data
```{r cleaning}
trulia_data <- read.csv(trulia_file, stringsAsFactors = F)
View(trulia_data)
trulia_df <- trulia_data %>% 
  transmute(sell_price = as.integer(str_replace_all(price, "\\$|\\+|,", "")),
         zip_code = as.character(zip_code)
         #neighborhood = str_replace(location, ", Chicago, IL", "")
         ) %>% 
  na.omit() %>% 
  group_by(zip_code) %>% 
  summarise(median_sell_price = median(sell_price),
            n = n())

glimpse(trulia_df)
hist(trulia_df$median_sell_price, breaks = 10)

#Data inspection functions
# options(scipen = 9)
# View(describe(trulia_df))
# #count NAs
# sapply(trulia_df, function(x) sum(is.na(x)))
# #count unique values
# sapply(trulia_df,  function(x) length(unique(x)))
# #value frequencies
# sapply(trulia_df[, 2:3], function(x) table(x))

```


##The Response Variable: Divvy Station Docks from City of Chicago API

Since stations can have differing numbers of docks, the number of Divvy station docks in each neighborhood/zip code. The source of these data will be the [City of Chicago's Divvy Bicycle Stations API](https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq).

###The Divvy API has 20 variables and 581 observations. Each observation is a Divvy station, and each station has a certain number of docks for bikes.
```{r api}

feed <- "https://feeds.divvybikes.com/stations/stations.json"
if (!exists("divvy_data")){  
  divvy_data <- fromJSON(feed)$stationBeanList
}
glimpse(divvy_data)

```


###The 2 variables needed are the `totalDocks` and `postalCode`. However, the `postalCode` values are mostly missing.
```{r nozips, comment="", collapse=T}
table(divvy_data$postalCode == "")
```

###Fortunately, the data set includes the longitude/latitude coordinates, and we can use ggmap to obtain the addresses from the Google Maps API.
```{r ggmap}
coordinates <- cbind(divvy_data$longitude, divvy_data$latitude)  
divvy_file <- "DivvyAddresses.csv"

if (!divvy_file %in% list.files(getwd()))
{  
  ##Code citation: http://stackoverflow.com/a/22919546
  address <- do.call(rbind,
                    lapply(1:nrow(coordinates),
                    function(i)revgeocode(coordinates[i, ])))
  write.csv(data.frame(address = address), file = divvy_file)
  }


divvy <- cbind(divvy_data, read.csv(divvy_file))
```


###Next, let's transform the data so that we have the number of bicycle docks in each zip code. One station not in service was removed.
```{r tidyup, tidy=F}

##one of the address values returned by ggmap is missing the zip code
missing_zip <- "730-798 W 28th St, Chicago, IL, USA"

divvy_df <- divvy %>% 
  mutate(zip_code = str_trim(str_extract(divvy$address, " [\\d]{5}"))) %>% 
  mutate(zip_code = ifelse(address == missing_zip, "60616", zip_code)) %>% 
  filter(status == "IN_SERVICE") %>%
  select(zip_code, totalDocks) %>%
  group_by(zip_code) %>%
  summarise(docks = sum(totalDocks))

kable(head(divvy_df))
glimpse(divvy_df)
```
##Combine the Data Sets
+ The response variable is left joined to the explanatory variable by zip code.
+ Note that zip codes not starting with "606" are not in the city of Chicago proper and were removed.


```{r combine}
divvy_trulia <- 
  left_join(trulia_df, divvy_df, by = "zip_code") %>% 
  filter(str_detect(zip_code, "606")) %>% 
  transmute(docks = ifelse(is.na(docks), 0, docks),
         median_sell_price_1000s = median_sell_price/1000,
         zip_code = zip_code)

glimpse(divvy_trulia)
View(divvy_trulia)
divvy_trulia$zip_code
```

#Inference
##Linear Regression Model

```{r}
bicycle_model <- lm(docks ~ median_sell_price_1000s, divvy_trulia)
summary(bicycle_model)

lm_coef <- coef(bicycle_model)
```


##Scatter Plot using ggplot2
```{r scatter}
a <- ggplot(divvy_trulia, aes(median_sell_price_1000s, docks))

a + geom_point() + geom_abline(aes(intercept = lm_coef[1], slope = lm_coef[2]))

```





##Model Diagnostics

Let's assess if this linear model is reliable

###Linearity

```{r residuals, eval=TRUE}

plot(bicycle_model$residuals ~ divvy_trulia$median_sell_price_1000s)
abline(h = 0, lty = 3) 
```

Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between runs and at-bats?
    
**There isn't any apparent pattern. It supports the linear condition.**
    
###Nearly normal residuals

```{r hist-res, eval=TRUE}
hist(bicycle_model$residuals)
```

```{r qq-res, eval=TRUE}
qqnorm(bicycle_model$residuals)
qqline(bicycle_model$residuals)  
```

Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met?
    **Yes**

###Constant variability*

Based on the plot in (1), does the constant variability condition appear to be met?
    
*Based upon the residual plot, there appears to be constant variability*



Statistical analysis: linear regression between the number of Divvy docks and the median sell price. There are 50 zip codes and about 90 neighborhoods in Chicago.

#Conclusion


