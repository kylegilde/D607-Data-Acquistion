---
title: "D607 Wk11 HW - Text Mining"
author: "Kyle Gilde"
date: "April 7, 2017"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

#The Task
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).  One example corpus:  https://spamassassin.apache.org/publiccorpus/


#Load Packages
```{r setup, echo = FALSE} 
knitr::opts_chunk$set(warning=FALSE, 
                      message=FALSE,
                      tidy=TRUE
                      )

#create vector with all needed packages
load_packages <- c("prettydoc", "SnowballC", "", "knitr", "RCurl", "rvest", "stringr", "tm", "RTextTools")

#see if we need to install any of them
install_load <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE)
}

install_load(load_packages)
#CODE SOURCE DOCUMENTATION: https://gist.github.com/stevenworthington/3178163
```

#Download files
```{r}
url <- "https://spamassassin.apache.org/publiccorpus/"
files <- url %>% 
  getURL() %>% 
  read_html() %>% 
  html_nodes("a") %>% 
  html_attr("href")

my_files <- files[c(9, 14)]
file_paths <- str_c(url, my_files)

for (i in 1:length(my_files))
{
  if (!my_files[i] %in% list.files(getwd()))
    download.file(file_paths[i], destfile = my_files[i])
}
```

#Load Files into Corpora
```{r}

#concat function
'%&%' <- function(x, y) paste0(x,y)

#Create the 2 corpora
data_dir <- "data-sets/spamham/"

spam_corpus <- VCorpus(DirSource(data_dir %&% "spam"), readerControl = list(language = "lat"))

ham_corpus <- VCorpus(DirSource(data_dir %&% "easy_ham"), readerControl = list(language = "lat"))

#add 2 metadata attributes to each & combine into one corpus
email_attr <- "email_type" 
attr_value1 <- "spam"
attr_value2 <- "ham"

group_attr <- "group"
attr_value_vec <- c("test", "train")

for (i in 1:length(spam_corpus))
{
  meta(spam_corpus[[i]], email_attr) <- attr_value1
  meta(spam_corpus[[i]], group_attr) <- sample(attr_value_vec, size = 1, prob = c(.5, .5), replace = T)
}

for (i in 1:length(ham_corpus))
{
  meta(ham_corpus[[i]], email_attr) <- attr_value2
  meta(ham_corpus[[i]], group_attr) <- sample(attr_value_vec, size = 1, prob = c(.5, .5), replace = T)
}
my_corpus <- c(ham_corpus, spam_corpus)
sort

meta(my_corpus)

lapply(my_corpus, meta)[1:10]
inspect(my_corpus[1:10])
meta(my_corpus[[2501]])
#Create DTM

dtm <- DocumentTermMatrix(release_corpus)
dtm <- removeSparseTerms(dtm, 1-(10/length(release_corpus)))
dtm

email_types <- unlist(meta(my_corpus, type = "local", tag = "email_type"))
group_list <- unlist(meta(my_corpus, type = "local", tag = "group"))
table(group_list)


N <- length(org_labels)
container <- create_container(
    dtm,
    labels = org_labels,
    trainSize = 1:400,
    testSize = 401:N,
    virgin = F
)

slotNames(container)


spam_corpus
ham_corpus[[1000]]$content


spam_corpus[[101]]$content
inspect(spam_corpus[1:10])
content(spam_corpus[1:10][1])
stopwords("en")[1:100]

```

