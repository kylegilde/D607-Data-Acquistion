---
title: "D607 Wk11 HW - Text Mining"
author: "Kyle Gilde"
date: "April 7, 2017"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---
#The Task
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).  

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      error = FALSE,
                      message = FALSE,
                      tidy = TRUE,
                      cache = TRUE)
```


#Loaded Packages
```{r load_packages, echo = FALSE, comment = ""} 
#create vector with all needed packages
load_packages <- c("prettydoc", "SnowballC", "knitr", "rvest", "stringr", "tm", "RTextTools", "caret", "dplyr")

#see if we need to install any of them
install_load <- function(pkg){
  #CODE SOURCE: https://gist.github.com/stevenworthington/3178163
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
}

#excute function and display the loaded packages
data.frame(t(t(install_load(load_packages))))
```

#Download & Extract Email Files
```{r download}
url <- "http://spamassassin.apache.org/old/publiccorpus/"

#parse the HTML
files <- url %>% 
  read_html() %>% 
  html_nodes("a") %>% 
  html_attr("href")

#concat function
'%&%' <- function(x, y) paste0(x,y)

#select the 2 files needed
my_files <- files[c(9, 14)]
file_urls <- url %&% my_files

#create temporary directory
data_dir <- "data-sets/spamham"
dir.create(data_dir)

#Download and unzip the doubly zipped files
for (i in 1:length(my_files))
{
  if (!my_files[i] %in% list.files(data_dir))
  {
      my_dest <- file.path(data_dir, my_files[i])
      download.file(file_urls[i], destfile = my_dest) 
      untar(bzfile(my_dest, "rb"), exdir = data_dir)
  }
    
}
```

##Let's take a look at the downloaded files & created directories
```{r files}
(afile <- list.files(data_dir))
head(list.files(data_dir %&% "/" %&% afile[3]))
```

#Create Corpus & Add Metadata
```{r create_corpora_metadata}
#Create the 2 corpora
spam_corpus <- VCorpus(DirSource(data_dir %&% "/" %&%  "spam_2"), readerControl = list(language = "lat"))

ham_corpus <- VCorpus(DirSource(data_dir %&% "/" %&%  "easy_ham"), readerControl = list(language = "lat"))

#add 2 metadata attributes
#email group attribute
email_attr <- "email_type" 
attr_value1 <- "spam"
attr_value2 <- "ham"

#randomly assign test and training group attribute values
group_attr <- "group"
attr_value_vec <- c("test", "train")

#loop through spam corpus and create metadata
for (i in 1:length(spam_corpus))
{
  meta(spam_corpus[[i]], email_attr) <- attr_value1
  meta(spam_corpus[[i]], group_attr) <- sample(attr_value_vec, size = 1, prob = c(.5, .5), replace = T)
}

#loop through ham corpus and create metadata
for (i in 1:length(ham_corpus))
{
  meta(ham_corpus[[i]], email_attr) <- attr_value2
  meta(ham_corpus[[i]], group_attr) <- sample(attr_value_vec, size = 1, prob = c(.5, .5), replace = T)
}

#combine corpora
my_corpus <- c(ham_corpus, spam_corpus)

email_types <- unlist(meta(my_corpus, type = "local", tag = "email_type"))
group_list <- unlist(meta(my_corpus, type = "local", tag = "group"))

table(email_types, group_list)
```

#Let's see how many ham & spam emails were assigned to the Test and Train groups.

```{r preview_metadata}
email_types <- unlist(meta(my_corpus, type = "local", tag = "email_type"))
group_list <- unlist(meta(my_corpus, type = "local", tag = "group"))

table(email_types, group_list)
```



#Clean my Corpus & Create a Document-Term Matrix with the Sparse Words Removed
```{r clean_corpus}
cleaned_corpus <- tm_map(my_corpus, removeNumbers)  
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(tolower)) 
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, words = stopwords("en")) 
cleaned_corpus <- tm_map(cleaned_corpus, stemDocument) 
cleaned_corpus <- tm_map(cleaned_corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)


#Create Document-Term Matrix
dtm <- DocumentTermMatrix(cleaned_corpus)
#remove sparse terms
dtm <- removeSparseTerms(dtm, 1-(10/length(cleaned_corpus)))

inspect(dtm) 
```

#Create Container, Train & Classify Models
```{r container_and_models}
training_indices <- which(group_list == "train")
test_indices <- which(group_list == "test")

container <- create_container(
    dtm,
    labels = email_types,
    trainSize = training_indices,
    testSize = test_indices,
    virgin = F
)

#slotNames(container)

# Train models
svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")

# Classify models
svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)

```


#Aggregate the Results of the 3 Models with the Actuals
```{r myresults}
# Construct data frame with correct labels
results <- data.frame(
    actuals = email_types[test_indices],
    svm = as.character(svm_out[,1]),
    tree = as.character(tree_out[,1]),
    maxent = as.character(maxent_out[,1]),
    stringsAsFactors = F
)
```

#Create Confusion Matrices

##Support Vector Machine Results
```{r svm_cm, comment = "", echo=FALSE}
svm_cm <- confusionMatrix(results$svm, results$actuals)
tree_cm <- confusionMatrix(results$tree, results$actuals)
maxent_cm <- confusionMatrix(results$maxent, results$actuals)

svm_cm
```


##Random Forest Results
```{r tree_cm, comment = "", echo=FALSE}
tree_cm
```


##Maximum Entropy Results
```{r maxent_cm, comment = "", echo=FALSE}
maxent_cm

n <- sum(group_list == "test")
n
```


#Conclusion
+ In this iteration, the test group had `r n` emails


```{r close_and_delete_data}
#Close connections & delete local data
# closeAllConnections()
# unlink(data_dir, recursive = T)

```

